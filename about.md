---
layout: page
title: About
permalink: /about/
---

## About This Journey

I'm Yashwanth Atla, and I'm embarking on an 8-month journey to deeply understand Artificial Intelligence and Large Language Models by building them from scratch.

### The Goal

Rather than just using AI tools, I want to understand how they work at a fundamental level. This blog documents my journey from basic neural network concepts to implementing transformer architectures and understanding RLHF (Reinforcement Learning from Human Feedback).

### My Approach

1. **Learn by Building**: Every concept is implemented from scratch
2. **Document Everything**: Share learnings, challenges, and insights
3. **Focus on Fundamentals**: Prioritize deep understanding over surface-level knowledge
4. **Daily Commitment**: Consistent progress through daily practice

### What I'm Learning

- Neural network fundamentals (neurons, activation functions, backpropagation)
- Language model architectures (from bigrams to transformers)
- Attention mechanisms and self-attention
- GPT architecture and implementation
- Training techniques and optimization
- RLHF and alignment methods

### Connect With Me

- **GitHub**: [yashuatla1431](https://github.com/yashuatla1431)
- **Research Repo**: [research-transformers](https://github.com/yashuatla1431/research-transformers)

### Inspiration

This journey was inspired by Andrej Karpathy's excellent tutorials and the belief that the best way to learn is by doing. Every blog post here represents not just reading about a concept, but implementing it, debugging it, and truly understanding it.

---

*Last updated: November 2025*
