---
layout: page
title: About
permalink: /about/
---

## About Me

I'm a Software Development Engineer at Valuefy with a deep interest in machine learning and deep learning. I'm currently focused on understanding the internals of modern AI systems—transformers, NLP, speech models, and reinforcement learning.

## Current Focus

Starting with NLP as my entry point, I'm working to master transformer architectures from the ground up. Recently, I built a GPT-2 124M parameter model from scratch, which was a significant milestone in understanding how these systems work at a fundamental level.

## Why This Blog

I document everything I learn here—from basic building blocks like bigram models to complex transformer architectures. The goal is to make these concepts accessible by breaking down the implementation details, the math, and the intuition behind them.

## What I'm Exploring

- Neural network fundamentals (neurons, activation functions, backpropagation)
- Language models (bigrams → transformers)
- Attention mechanisms and multi-head attention
- GPT architecture and training
- Reinforcement Learning from Human Feedback (RLHF)

## How I Write

I focus on explaining concepts and architecture at a high level while showing the complete code. I explain what each code block does conceptually—the big picture—rather than documenting every line. For detailed function-level understanding, the code is there for you to explore.

## Links

- [GitHub](https://github.com/yashuatla1431)
- [Research Repo](https://github.com/yashuatla1431/research-transformers)
- yashwanthatla1431@gmail.com
