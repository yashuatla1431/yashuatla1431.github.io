# AI Research Journey Blog

A personal blog documenting my 8-month journey to understand Transformers, RLHF, and modern Large Language Models from first principles.

## About

This blog chronicles my path from neural network fundamentals to implementing advanced transformer architectures. Each post represents hands-on learning through building and implementing concepts from scratch.

## Features

- **Custom Design**: Beautiful, modern styling with a clean, professional look
- **Responsive Layout**: Works seamlessly on desktop and mobile devices
- **Code Highlighting**: Syntax-highlighted code blocks for better readability
- **Organized Content**: Posts categorized by topic (fundamentals, transformers, hands-on)
- **Project Integration**: Links to my research repository

## Local Development

To run this blog locally:

```bash
# Install dependencies
bundle install

# Serve the site
bundle exec jekyll serve

# Visit http://localhost:4000 in your browser
```

## Content Categories

- **Fundamentals**: Core concepts like neurons, activation functions, and neural networks
- **Language Models**: From bigram models to transformers
- **Transformers**: GPT architecture and attention mechanisms
- **Hands-On**: Practical implementation guides and code walkthroughs
- **Project Log**: Daily updates on research progress

## Tech Stack

- **Jekyll**: Static site generator
- **Minima Theme**: Customized with extensive CSS overrides
- **GitHub Pages**: Hosting
- **Markdown**: Content format
- **Rouge**: Syntax highlighting

## Project Repository

The code implementations discussed in this blog can be found at:
[research-transformers](https://github.com/yashuatla1431/research-transformers)

## License

Content is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)

---

*Built with Jekyll and hosted on GitHub Pages*
